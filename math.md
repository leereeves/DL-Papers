# Machine Learning: the math

Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of training deep feedforward neural networks." Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010.

Studying gradients and their propogation, with application to proper initialization.

http://proceedings.mlr.press/v9/glorot10a


He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification." Proceedings of the IEEE international conference on computer vision. 2015.

Why and how to initialize layers with ReLU activation.

https://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html

Pennington, Jeffrey, Samuel Schoenholz, and Surya Ganguli. "The emergence of spectral universality in deep networks." International Conference on Artificial Intelligence and Statistics. PMLR, 2018.

Attempting to "build a full theoretical understanding of the spectra of Jacobians at initialization"

https://proceedings.mlr.press/v84/pennington18a.html

Xiao, Lechao, et al. "Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks." International Conference on Machine Learning. PMLR, 2018.

Studying signal propogation and proper initialization of deep CNNs

http://proceedings.mlr.press/v80/xiao18a

Zhang, Hongyi, Yann N. Dauphin, and Tengyu Ma. "Fixup initialization: Residual learning without normalization." arXiv preprint arXiv:1901.09321 (2019).

Why gradients in residual networks explode and how to fix it without batch normalization.

https://arxiv.org/abs/1901.09321
